{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract UNWTO Tourist estimates and use them to create adjustment factors for Alcohol LPC\n",
    "### By Max Griswold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up global variables and file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import string\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "#Starting datasets\n",
    "raw = {}\n",
    "\n",
    "visitors_by_residence = {}\n",
    "visitors_by_nationality = {}\n",
    "tourists_by_residence = {}\n",
    "tourists_by_nationality = {}\n",
    "filtered_by_cp = {}\n",
    "\n",
    "proportion_datasets = [['visitors_by_nationality', visitors_by_nationality], ['visitors_by_residence', visitors_by_residence], ['tourists_by_nationality', tourists_by_nationality], ['tourists_by_residence', tourists_by_residence]]\n",
    "years = list(range(1995, 2015, 1))\n",
    "\n",
    "##File locations\n",
    "incoming = '/home/j/DATA/UNWTO_COMPENDIUM_TOURISM_STATISTICS/1995_2014'\n",
    "outgoing = \"/home/j/05_risk/risks/drugs_alcohol/data/exp/inputs\"\n",
    "\n",
    "template = pd.read_stata('/home/j/WORK/05_risk/risks/drugs_alcohol/data/exp/inputs/alc_template.dta')\n",
    "template.rename(columns={'iso3':'ihme_loc_id'}, inplace=True)\n",
    "template.drop_duplicates(['ihme_loc_id', 'year_id'])\n",
    "\n",
    "alcohol_lpc = '/home/j/WORK/05_risk/risks/drugs_alcohol/data/exp/stgpr/alcohol_lpc.dta'\n",
    "\n",
    "#Variables used to handle draws later from alc lpc.\n",
    "draws = []\n",
    "for i in range(1000):\n",
    "    draws.append('draw_{}'.format(i))\n",
    "\n",
    "draws_old = []\n",
    "for i in range(1000):\n",
    "    draws_old.append('draw_{}_old'.format(i))\n",
    "\n",
    "rename = dict(zip(draws, draws_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read files from incoming folder, set name of dataframe to specific countries and store in dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for files in os.listdir(incoming):\n",
    "    if files.lower().endswith('.xlsx'):\n",
    "        name = files.split('UNWTO_COMPENDIUM_TOURISM_STATISTICS_1995_2014_')\n",
    "        name = name[1].split('_Y2016M01D12.XLSX')\n",
    "        name = name[0].replace(\"_\", \" \")\n",
    "        name = name.lower()\n",
    "        name = string.capwords(name)\n",
    "        data = pd.read_excel(incoming + '/' + files, sheetname=None, header=3, skiprows=2, na_values= ['..', '', 0], keep_default_na = False)\n",
    "        raw[name] = data\n",
    "\n",
    "#Make function to fix UNWTO location names not conforming to GBD location names\n",
    "def fix_locations(data, more=False):\n",
    "    location_fix = {'Antigua And Barbuda':'Antigua and Barbuda', 'Bahamas':'The Bahamas', 'Bolivia, Plurinational State Of': 'Bolivia', 'Bosnia And Herzegovina':'Bosnia and Herzegovina', 'Brunei Darussalam':'Brunei', 'Congo, Democratic Republic Of The':'Democratic Republic of the Congo', 'Cote D Ivoire':\"Cote d'Ivoire\", 'Gambia':'The Gambia', 'Guinea-bissau':'Guinea-Bissau', 'Hong Kong, China':'Hong Kong Special Administrative Region of China', 'Iran, Islamic Republic Of':'Iran', 'Korea, Republic Of':'South Korea', 'Macao China':'Macao Special Administrative Region of China', 'Micronesia, Federated States Of':'Federated States of Micronesia', 'Russian Federation':'Russia', 'Saint Vincent And The Grenadines':'Saint Vincent and the Grenadines', 'Sao Tome And Principe':'Sao Tome and Principe', 'State Of Palestine':'Palestine', 'Syrian Arab Republic':'Syria', 'Tanzania, United Republic Of':'Tanzania', 'Timor Leste':'Timor-Leste', 'Trinidad And Tobago':'Trinidad and Tobago', 'United States Of America':'United States', 'United States Virgin Islands':'Virgin Islands, U.S.', 'Venezuela, Bolivarian Republic Of':'Venezuela', 'Viet Nam':'Vietnam'}\n",
    "    visitors_fix = {'Bahamas':'The Bahamas', 'Belgium / Luxembourg':'Belgium', 'Bolivia, Plurinational State of': 'Bolivia', 'Brunei Darussalam':'Brunei', 'China + Hong Kong, China':'China', 'Congo, Democratic Republic of the':'Democratic Republic of the Congo', \"CÃ´te d'Ivoire\":\"Code d'Ivoire\", 'Czech Republic/Slovakia':'Czech Republic', 'Gambia':'The Gambia', 'Hong Kong, China':'Hong Kong Special Administrative Region of China', 'India, Pakistan':'India', 'Iran, Islamic Republic of':'Iran', 'Korea, Republic of':'South Korea', \"Korea, Democratic People's Republic of\":'South Korea', \"Lao People's Democratic Republic\":'Laos', 'Macao, China':'Macao Special Administrative Region of China', 'Micronesia, Federated States of':'Federated States of Micronesia', 'Russian Federation':'Russia', 'Spain,Portugal':'Spain', 'State of Palestine':'Palestine', 'Syrian Arab Republic':'Syria', 'United Kingdom/Ireland':'United Kingdom', 'Taiwan Province of China':'Taiwan', 'Tanzania, United Republic of':'Tanzania', 'United States of America':'United States', 'United States Virgin Islands':'Virgin Islands, U.S.', 'Venezuela, Bolivarian Republic of':'Venezuela', 'Viet Nam':'Vietnam'}\n",
    "    for unwto, gbd in location_fix.items():\n",
    "        data['location_name'][data['location_name'] == unwto] = gbd\n",
    "    if more == True:\n",
    "        for unwto, gbd in location_fix.items():\n",
    "            data['visiting_country'][data['visiting_country'] == unwto] = gbd\n",
    "        for unwto, gbd in visitors_fix.items():\n",
    "            data['visiting_country'][data['visiting_country'] == unwto] = gbd\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read specific sheets from raw datasets and store in filtered dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for country, dataset in raw.items():\n",
    "    for sheet, data in dataset.items():\n",
    "        if sheet == '121':\n",
    "            proportion_datasets[0][1][country] = data\n",
    "        if sheet == '122':\n",
    "            proportion_datasets[1][1][country] = data\n",
    "        if sheet == '111':\n",
    "            proportion_datasets[2][1][country] = data\n",
    "        if sheet == '112':\n",
    "            proportion_datasets[3][1][country] = data\n",
    "        if sheet == 'CP':\n",
    "            filtered_by_cp[country] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Append filtered dictionaries, then pivot to create individual variables. Then transform to proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/inspect.py\", line 1048, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/inspect.py\", line 1008, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/py/_apipkg.py\", line 171, in __getattribute__\n",
      "    return getattr(getmod(), name)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/py/_apipkg.py\", line 155, in getmod\n",
      "    x = importobj(modpath, None)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/py/_apipkg.py\", line 48, in importobj\n",
      "    module = __import__(modpath, None, None, ['__doc__'])\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/pytest.py\", line 27, in <module>\n",
      "    _preloadplugins() # to populate pytest.* namespace so help(pytest) works\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/_pytest/config.py\", line 109, in _preloadplugins\n",
      "    _preinit.append(get_config())\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/_pytest/config.py\", line 118, in get_config\n",
      "    pluginmanager.import_plugin(spec)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/_pytest/config.py\", line 431, in import_plugin\n",
      "    __import__(importspec)\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/_pytest/python.py\", line 17, in <module>\n",
      "    from _pytest import fixtures\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/_pytest/fixtures.py\", line 3, in <module>\n",
      "    from py._code.code import FormattedExcinfo\n",
      "  File \"/homes/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/py/_code/__init__.py\", line 1, in <module>\n",
      "    \"\"\" python inspection/code generation API \"\"\"\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_filtered(filtered, data_name):\n",
    "    '''Extract data from filtered dictionaries.\n",
    "       Only keeps data on countries and relevant variables. Renames variables to\n",
    "       match template.\n",
    "    '''\n",
    "    copy = DataFrame()\n",
    "    merger = DataFrame()\n",
    "    frames = {'total':[], 'visiting':[]}\n",
    "\n",
    "    for country, host in filtered.items():\n",
    "\n",
    "        #Only keep countries not regions, as well as useful indicators\n",
    "        host_clean = host[(host['CODE'] < 900) | (host['CODE'].isnull())]\n",
    "        host_clean = host_clean.drop(['CODE', '% Change 2014-2013', 'Notes', 'NOTES'], axis=1)\n",
    "\n",
    "        #Rename columns and convert to long\n",
    "        host_clean = host_clean.rename(columns = {'Unnamed: 2':'visiting_country'})\n",
    "\n",
    "        #Get rid of pesky marketshare category\n",
    "        host_clean = host_clean.iloc[:,:-1]\n",
    "\n",
    "        host_clean['location_name'] = '{}'.format(country)\n",
    "        host_clean = pd.melt(host_clean, id_vars=['REGION', 'location_name', 'visiting_country'], var_name='year_id', value_name=data_name)\n",
    "\n",
    "        #Keep total separate from visiting countries\n",
    "        frames['total'].append(host_clean[host_clean['visiting_country'] == 'TOTAL'])\n",
    "        frames['visiting'].append(host_clean[host_clean['visiting_country'] != 'TOTAL'])\n",
    "\n",
    "    #Merge visiting countries for host, keeping total separate\n",
    "    merger = pd.concat(frames['total'], ignore_index=True)\n",
    "    copy = pd.concat(frames['visiting'], ignore_index=True)\n",
    "\n",
    "    #Merge location specific with totals\n",
    "    merger = merger.drop(['visiting_country', 'REGION'], axis=1)\n",
    "    merger = merger.rename(columns = {data_name:'Total'})\n",
    "    copy = pd.merge(copy, merger, how='left', on=['location_name', 'year_id'], sort=False)\n",
    "\n",
    "    #Make sure missing observations are coded as NaN and that only countries are kept, not NaN\n",
    "    for row in copy.index:\n",
    "        if type(copy.iloc[row, -1]) == str:\n",
    "            copy.iloc[row, -1] = np.nan\n",
    "        if type(copy.iloc[row, -2]) == str:\n",
    "            copy.iloc[row, -2] = np.nan\n",
    "\n",
    "    #Get rid of countries with name NaN\n",
    "    copy = copy[copy['visiting_country'] == copy['visiting_country']]\n",
    "    return copy\n",
    "\n",
    "proportion_datasets_clean=[]\n",
    "\n",
    "#Merge each category group from data extracted\n",
    "for dataset in range(len(proportion_datasets)):\n",
    "    proportion_datasets_clean.append(merge_filtered(proportion_datasets[dataset][1], '{}'.format(proportion_datasets[dataset][0])))\n",
    "\n",
    "    #Generate logged tourist proportions\n",
    "    proportion_datasets_clean[dataset]['tourist_proportion'] = np.log(proportion_datasets_clean[dataset].iloc[:,-2]/proportion_datasets_clean[dataset].iloc[:,-1])\n",
    "    proportion_datasets_clean[dataset] = proportion_datasets_clean[dataset][['location_name', 'visiting_country', 'year_id', 'tourist_proportion']]\n",
    "    proportion_datasets_clean[dataset].sort_values(['location_name', 'visiting_country', 'year_id'], inplace=True)\n",
    "\n",
    "#Merge all proportions on together    \n",
    "i=1\n",
    "for data in proportion_datasets_clean:\n",
    "    data.rename(columns={'tourist_proportion':'tourist_proportion_{}'.format(i)}, inplace=True)\n",
    "    data.set_index(['location_name', 'visiting_country', 'year_id'], inplace=True)\n",
    "    i+=1\n",
    "\n",
    "combine = pd.concat(proportion_datasets_clean, axis=1, join='outer')\n",
    "combine['tourist_proportions'] = combine.mean(axis=1)\n",
    "combine.reset_index(inplace=True)\n",
    "combine = combine[['year_id', 'location_name', 'visiting_country', 'year_id', 'tourist_proportions']]\n",
    "combine.to_csv(\"C:/users/mgriswol/desktop/tourist_proportions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Estimate full time series for tourist_proportions using amelia & mice regression\n",
    "\n",
    "See R Code  \n",
    "Proportion datasets were concatted together after running above cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(data.table)\n",
    "library(plyr)\n",
    "\n",
    "dir <- \"/share/scratch/users/mgriswol/\"\n",
    "\n",
    "#Load tourist data\n",
    "df <- fread(paste0(dir, \"tourist_proportions.csv\"))\n",
    "df[, id:=paste(df$location_name, df$visiting_country, sep=\",\")]\n",
    "\n",
    "#Load covariate data\n",
    "covar <- fread(paste0(dir, \"covariates.csv\"))\n",
    "covar[, cigarettes_pc:=NULL]\n",
    "\n",
    "#Load alcohol predictions\n",
    "alc <- fread(paste0(dir, \"alc.csv\"))\n",
    "alc <- melt(alc, id.vars=c(\"location_id\", \"location_name\", \"year_id\", \"me_name\"))\n",
    "alc <- alc[, mean(value, na.rm=T), by=c(\"location_name\", \"year_id\")]\n",
    "setnames(alc, \"V1\", \"alc_lpc\")\n",
    "\n",
    "#Combine covariates with alcohol, merge onto tourist data.\n",
    "predictors <- join(covar, alc, by=c(\"location_name\", \"year_id\"), type=\"left\")\n",
    "predictors[, data:=NULL]\n",
    "predictors <- predictors[!duplicated(predictors),]\n",
    "\n",
    "final <- join(df, predictors, by=c(\"location_name\", \"year_id\"), type=\"left\")\n",
    "\n",
    "#Make location id\n",
    "locations <- data.table(location_id = seq(1, length(unique(final$location_name))), location_name = unique(final$location_name))\n",
    "\n",
    "final <- join(final, locations, by=\"location_name\", type=\"left\")\n",
    "\n",
    "#Overimpute for observed cases, parallelized on cluster\n",
    "write.csv(final, paste0(dir, \"final.csv\"))\n",
    "R_shell <- paste0(dir,\"r_shell.sh\")\n",
    "error_path <- paste0(\" -o /share/scratch/users/mgriswol/\", \" -e /share/scratch/users/mgriswol/\")\n",
    "\n",
    "for (loc in locations$location_id){\n",
    "  \n",
    "  #Build qsub\n",
    "  name <- paste0(\"impute_case_\",loc)\n",
    "  script <- paste0(dir,\"impute.R\")\n",
    "  arguments <- paste(dir, loc)\n",
    "  \n",
    "  qsub <- paste0(\"qsub -N \", name, \n",
    "                 \" -pe multi_slot \", 2,\n",
    "                 \" -l mem_free=\", 4,\n",
    "                 error_path\n",
    "                 )\n",
    "  #Run qsub\n",
    "  system(paste(qsub, R_shell, script, arguments))\n",
    "  #print(paste(qsub, R_shell, script, arguments))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#Loop through cases\n",
    "\n",
    "library(data.table)\n",
    "library(Amelia)\n",
    "\n",
    "arg <- commandArgs()[-(1:3)]\n",
    "\n",
    "dir <- arg[1]\n",
    "loc <- arg[2]\n",
    "\n",
    "print(loc)\n",
    "print(dir)\n",
    "\n",
    "df <- fread(paste0(dir, \"final.csv\"))\n",
    "df[, V1:=NULL]\n",
    "df <- df[location_id==loc,]\n",
    "\n",
    "#Check that covariates exist\n",
    "for (covariate in c(\"ln_ldi_pc\", \"educ_25plus\", \"alc_lpc\")){\n",
    "  if (nrow(df[is.na(get(covariate)), ])>=0){\n",
    "    df[, (covariate):=NULL]\n",
    "  }\n",
    "}\n",
    "\n",
    "a <- amelia(df, m=10, \n",
    "            cs=\"id\", \n",
    "            ts=\"year_id\", \n",
    "            p2s = 2,\n",
    "            idvars=c(\"location_name\", \"visiting_country\", \"location_id\"), \n",
    "            polytime = 1, \n",
    "            intercs = T,\n",
    "            empri = 0.01*dim(df)[1],\n",
    "            parallel = \"multicore\")\n",
    "\n",
    "write.amelia(a, separate = FALSE, orig.data = TRUE, file.stem = paste0(dir, \"imputed_\", loc), impvar = \"imputed\", extension = \".csv\", format=\"csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(data.table)\n",
    "library(plyr)\n",
    "\n",
    "folder <- \"/share/scratch/users/mgriswol/tourism_imputed/\"\n",
    "setwd(folder)\n",
    "\n",
    "#Read in only the files we want and append together in a datatable\n",
    "files <- list.files(folder, pattern=\"^imputed.*csv$\")\n",
    "impute <- lapply(files, fread)\n",
    "impute <- data.table(rbindlist(impute))\n",
    "\n",
    "#Remove some columns and transform data\n",
    "impute[, c(\"V1\", \"id\", \"location_id\"):=NULL]\n",
    "impute[, tourist_proportions:=exp(tourist_proportions)]\n",
    "\n",
    "#Scale proportions to 1\n",
    "impute[, tourist_proportions := tourist_proportions/sum(.SD[,tourist_proportions], na.rm=TRUE), \n",
    "           by=list(location_name, year_id, imputed)]\n",
    "\n",
    "#Split data into raw data and estimates\n",
    "data <- impute[imputed==0,]\n",
    "data[, imputed:=NULL]\n",
    "\n",
    "impute <- impute[imputed!=0,]\n",
    "\n",
    "#Calculate estimates and uncertainty\n",
    "impute[, `:=`(mean = mean(tourist_proportions),\n",
    "             lower = quantile(tourist_proportions, 0.05),\n",
    "             upper = quantile(tourist_proportions, 0.95)),\n",
    "       by=.(location_name, visiting_country, year_id)]\n",
    "\n",
    "#Collapse dataset and compare to original data\n",
    "impute[, c(\"tourist_proportions\", \"imputed\"):=NULL]\n",
    "impute <- unique(impute)\n",
    "\n",
    "data[, imputed:=NULL]\n",
    "results <- join(data, impute, by=c(\"location_name\", \"visiting_country\", \"year_id\"), type=\"full\")\n",
    "\n",
    "write.csv(results, \"/share/scratch/users/mgriswol/tourism_imputed/final_tourist_proportions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snfs2/HOME/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/snfs2/HOME/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/snfs2/HOME/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Combine all of the best datasets for each country pair. \n",
    "\n",
    "#Replace frames with Amelia output\n",
    "tourist_proportions = pd.read_csv(\"/share/scratch/users/mgriswol/tourism_imputed/final_tourist_proportions.csv\")\n",
    "\n",
    "#Format so that dataset merges correctly with later datasets\n",
    "fix = fix_locations(tourist_proportions, more=True)\n",
    "fix.rename(columns={'location_name':'host', 'visiting_country':'location_name'}, inplace=True)\n",
    "fix = pd.merge(fix, template, how='left', on=['location_name', 'year_id'])\n",
    "fix = fix[['year_id', 'host', 'location_name', 'mean', 'lower', 'upper', 'location_id']]\n",
    "fix.rename(columns={'host':'location_name', 'location_name':'visiting_country', 'location_id':'location_id_visitor'}, inplace=True)\n",
    "\n",
    "tourist_proportions = fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split cp by filtered proportions predictions to produce full time series for tourism for all countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snfs2/HOME/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "games = []\n",
    "\n",
    "def merge_cp(data, country):\n",
    "    '''Returns cleaned dataset for total tourism'''\n",
    "\n",
    "    #Only keep relevant variables, rename, and transform the units.\n",
    "    copy = data.iloc[[4, 5, 6, 54]]\n",
    "    copy = copy.drop(['Cod.', 'Notes', 'Units'], axis=1)\n",
    "    copy.iloc[0,0] = 'tourist_total'\n",
    "    copy.iloc[1,0] = 'overnight_visitors'\n",
    "    copy.iloc[2,0] = 'same_day_visitors'\n",
    "    copy.iloc[3,0] = 'length_of_stay'\n",
    "    copy.iloc[:-1,1:] = copy.iloc[:-1,1:]*1000\n",
    "\n",
    "    #Reshape by years, then make separate columns\n",
    "    copy = pd.melt(copy, id_vars=['Basic data and indicators'], var_name='year_id', value_name='data')\n",
    "    copy = copy.pivot(index='year_id', columns='Basic data and indicators', values='data')\n",
    "    copy['location_name'] = country\n",
    "    copy['year_id'] = copy.index\n",
    "\n",
    "    #Replace almost all missing values with next best estimates\n",
    "    copy = next_best(copy)\n",
    "    return copy\n",
    "\n",
    "def next_best(data):\n",
    "    '''Replaces tourist total with next best estimate if tourist total is missing'''\n",
    "\n",
    "    if np.isnan(data['tourist_total'].values).sum() >= 19:\n",
    "        data['tourist_total'] = data['overnight_visitors']\n",
    "    if np.isnan(data['tourist_total'].values).sum() >= 19:\n",
    "        data['tourist_total'] = data['same_day_visitors']\n",
    "    return data\n",
    "\n",
    "for country, data in filtered_by_cp.items():\n",
    "    games.append(merge_cp(data, country))\n",
    "\n",
    "#Merge on stgpr template\n",
    "tourist_total = pd.concat(games, ignore_index=True)\n",
    "tourist_total = fix_locations(tourist_total)\n",
    "tourist_total_gpr = pd.merge(tourist_total, template, how='right', on=['location_name', 'year_id'])\n",
    "tourist_total_gpr['year_id'] = tourist_total_gpr['year_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prep tourist_total for ST-GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowess(data, fraction):\n",
    "    '''Calculates lowess and returns predictions'''\n",
    "\n",
    "    x = data['year_id']\n",
    "    y = data['data']\n",
    "\n",
    "    prediction = sm.nonparametric.lowess(y, x, frac=fraction, it=10, missing='drop', return_sorted=False)\n",
    "\n",
    "    return(prediction)\n",
    "\n",
    "#Rename columns for gpr\n",
    "tourist_total_gpr.rename(columns={'tourist_total':'data'}, inplace=True)\n",
    "\n",
    "#Generate variance and SD using difference from lowess estimates\n",
    "grouped = tourist_total_gpr.groupby('ihme_loc_id')\n",
    "dames = []\n",
    "\n",
    "for country, data in grouped:\n",
    "\n",
    "    #Only run lowess for models with atleast 2 data points. For those with a small amount, use all of the data.\n",
    "    check = data['data'].values\n",
    "    check = np.isnan(check)\n",
    "\n",
    "    if sum(~check) >= 7:\n",
    "        lowess_hat = lowess(data, .6)\n",
    "        data['lowess_hat'] = lowess_hat\n",
    "        dames.append(data)\n",
    "    if sum(~check) >= 4 and sum(~check) <= 6:\n",
    "        lowess_hat = lowess(data, 1)\n",
    "        data['lowess_hat'] = lowess_hat\n",
    "        dames.append(data)\n",
    "\n",
    "lowess_hat = pd.concat(dames, ignore_index=True)\n",
    "lowess_hat = lowess_hat[['location_name', 'year_id', 'data', 'lowess_hat']]\n",
    "lowess_hat.sort_values(['location_name', 'year_id'], inplace=True)\n",
    "\n",
    "#Collect lowess predictions with gpr prep dataset\n",
    "lowess_hat = lowess_hat[['location_name', 'year_id', 'lowess_hat']]\n",
    "gpr = pd.merge(tourist_total_gpr, lowess_hat, on=['location_name', 'year_id'], how='left')\n",
    "\n",
    "gpr['residual'] = gpr['lowess_hat'] - gpr['data']\n",
    "\n",
    "#By country, use difference between lowess estimates and data to generate variance over a 5 year window\n",
    "grouped = gpr.groupby('location_name')\n",
    "frames=[]\n",
    "\n",
    "for location, data in grouped:\n",
    "    data['standard_deviation'] = pd.rolling_std(data['residual'], window=5, center=True, min_periods=1)\n",
    "    frames.append(data)\n",
    "\n",
    "gpr = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "#Only hold onto variance at points where we have data. (This happens due to the rolling window)\n",
    "gpr['standard_deviation'][gpr['residual'] != gpr['residual']] = np.nan\n",
    "gpr['variance'] = gpr['standard_deviation']**2\n",
    "\n",
    "gpr['constant'] = 1\n",
    "\n",
    "#Add missing China subnational\n",
    "china = template[template['location_name']=='China']\n",
    "china['ihme_loc_id'] = 'CHN_44533'\n",
    "china['location_id'] = 44533\n",
    "china['location_name'] = 'CHN_44533'\n",
    "gpr = pd.merge(gpr, china, on=['location_id', 'year_id'], how='left')\n",
    "\n",
    "#Add on last columns needed for gpr\n",
    "gpr['nid'] = 239757\n",
    "gpr['me_name'] = 'total_tourists'\n",
    "gpr['sample_size'] = np.nan\n",
    "gpr['sex_id'] = 3\n",
    "gpr['age_group_id'] = 22\n",
    "gpr['age_id'] = 22\n",
    "\n",
    "gpr.to_csv(r'J:/WORK/05_risk/risks/drugs_alcohol/data/exp/inputs/total_tourists_pre_gpr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bring in GPR results on LPC and use this, along with transformed tourism data, to create tourism adjustments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following equations describe the adjustment:\n",
    "\n",
    "$$ \\text{Adjusted LPC}_h = \\text{Observed LPC}_h + \\text{LPC}_\\textit{a} - \\text{LPC}_v $$\n",
    "\n",
    "$$ \\text{LPC}_a = \\frac{\\sum_v (\\text{Unadjusted LPC}_h * \\text{Tourist pop}_\\text{a,v})}{\\text{Population}_h}$$\n",
    "\n",
    "$$ \\text{LPC}_v =  \\frac{\\sum_v (\\text{Unadjusted LPC}_v * \\text{Tourist pop}_\\text{h,v})}{\\text{Population}_h}$$\n",
    "\n",
    "$$ \\text{Tourist pop}_\\text{i,v} = \\text{Proportion of tourists}_{i,v} * \\text{Tourist pop}_v * \\frac{\\text{Trip duration}_{i,v}}{365} \\; \\; \\text{for }i={a,h}$$  \n",
    "\n",
    "where h is a hosting country, a is citizens from a hosting country traveling abroad, and v is a visiting country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snfs2/HOME/mgriswol/.conda/envs/my_python/lib/python2.7/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Read in alcohol lpc gpr results and organize columns\n",
    "\n",
    "alc_lpc = pd.read_csv(\"/snfs1/WORK/05_risk/risks/drugs_alcohol/data/exp/stgpr/alc_gpr_2016.csv\")\n",
    "alc_lpc = alc_lpc[['year_id','location_id']+draws]\n",
    "alc_lpc = alc_lpc[alc_lpc['year_id'] >= 1995]\n",
    "alc_lpc = pd.merge(alc_lpc, template, how='left', on=['location_id', 'year_id'])\n",
    "alc_lpc.rename(columns={'location_name':'visiting_country', 'location_id':'location_id_visitor'}, inplace=True)\n",
    "alc_lpc = pd.melt(alc_lpc, id_vars=['year_id', 'location_id_visitor', 'visiting_country'], value_vars=draws, var_name=\"draw\", value_name=\"alc_lpc\")\n",
    "\n",
    "#Read in tourist proportions and merge with alc lpc\n",
    "tourism_statistics = pd.merge(alc_lpc, tourist_proportions, how='inner', on=['visiting_country', 'location_id_visitor', 'year_id'])\n",
    "tourism_statistics = tourism_statistics[['year_id', 'location_name', 'mean', 'location_id_visitor', 'visiting_country', 'draw', 'alc_lpc']]\n",
    "tourism_statistics.sort_values(['location_name', 'year_id', 'location_id_visitor'], inplace=True)\n",
    "tourism_statistics['year_id'] = tourism_statistics['year_id'].astype(int)\n",
    "tourism_statistics.rename(columns={'mean':'tourist_proportion'}, inplace=True)\n",
    "\n",
    "#Read in total tourists\n",
    "\n",
    "total_tourists = pd.read_csv(r'/snfs1/WORK/05_risk/risks/drugs_alcohol/data/exp/stgpr/total_tourists.csv')\n",
    "total_tourists = total_tourists[total_tourists['year_id'] >=1995]\n",
    "total_tourists = pd.merge(total_tourists, template, on=['ihme_loc_id', 'location_id', 'year_id'], how='left')\n",
    "total_tourists = total_tourists[['location_id', 'location_name', 'year_id', 'gpr_mean']]\n",
    "total_tourists.rename(columns={'gpr_mean':'total_tourists'}, inplace=True)\n",
    "\n",
    "#Calculate trip duration/365, merge onto total_tourists\n",
    "\n",
    "length = tourist_total[['length_of_stay', 'location_name', 'year_id']]\n",
    "total_tourists = pd.merge(total_tourists, length, on=['location_name', 'year_id'], how='left')\n",
    "total_tourists['length_of_stay'][total_tourists['length_of_stay'] != total_tourists['length_of_stay']] = 10\n",
    "total_tourists['length_of_stay'] = total_tourists['length_of_stay']/365\n",
    "\n",
    "#Get populations\n",
    "\n",
    "pop = template[['location_name', 'year_id', 'pop_scaled', 'location_id']]\n",
    "tourism_statistics = pd.merge(tourism_statistics, pop, on=['location_name', 'year_id'], how='left')\n",
    "\n",
    "#Merge total tourists with tourist proportions and alcohol consumption\n",
    "alc_lpc_tourists = pd.merge(tourism_statistics, total_tourists, on=['location_id', 'location_name', 'year_id'], how='left')\n",
    "\n",
    "#Sort and rename columns to make operations below clearer:\n",
    "tourism_inputs = alc_lpc_tourists[['location_name', 'location_id', 'visiting_country', 'location_id_visitor', 'year_id', 'draw', 'alc_lpc', 'total_tourists', 'tourist_proportion', 'length_of_stay']]\n",
    "tourism_inputs.rename(columns={'location_name':'host_country', 'location_id':'location_id_host'}, inplace=True)\n",
    "\n",
    "#Memory is getting large so delete some objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for var, obj in locals().items():\n",
    "    print var, sys.getsizeof(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate tourist populations, numerators, and host populations\n",
    "tourism_inputs['tourist_pop'] = (tourism_inputs['total_tourists']*tourism_inputs['length_of_stay']*tourism_inputs['tourist_proportion'])\n",
    "tourism_inputs['numerator'] = tourism_inputs['alc_lpc']*(tourism_inputs['tourist_pop'])\n",
    "\n",
    "#Calculate domestic citizen's consumption abroad (additive measure)\n",
    "tourism_inputs['add'] = tourism_inputs.groupby(['location_id_visitor', 'visiting_country', 'year_id', 'draw'])['numerator'].transform(sum)\n",
    "tourism_inputs = pd.merge(tourism_inputs, pop, how='left', left_on=['location_id_visitor', 'year_id'], right_on=['location_id', 'year_id'])\n",
    "\n",
    "#Calculate tourist consumption domestically (subtractive measure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tourism_inputs.drop('population_host', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%Backcast average estimates of last three years to the years 1980-1989 to match the covariate, along with forecasting to 2016\n",
    "#(Better method would be to account for changes in host country populations in addition to using this average,\n",
    "#i.e. only divide by pop_scaled after having back and forecasted values)\n",
    "\n",
    "backcast = template[['location_id', 'location_name', 'year_id']]\n",
    "backcast = backcast[(backcast['year_id'] <= 1994) | (backcast['year_id'] >= 2015)]\n",
    "backcast = backcast.groupby('location_id')\n",
    "\n",
    "grouped = sub.groupby('location_id')\n",
    "frames = []\n",
    "\n",
    "for country, dataset in grouped:\n",
    "    dataset = dataset.append(backcast.get_group(country))\n",
    "    avg = dataset[draws][(dataset['year_id'] >= 1995) & (dataset['year_id'] <= 1997)].mean()\n",
    "    dataset[draws].fillna(avg, inplace=True)\n",
    "    dataset[draws][dataset['year_id'] == 2015] = dataset[draws][dataset['year_id'] == 2014]\n",
    "    frames.append(dataset)\n",
    "\n",
    "sub = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Combine with alc_lpc gpr results and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alc_lpc = pd.read_stata(alcohol_lpc)\n",
    "alc_lpc.rename(columns=rename, inplace=True)\n",
    "alc_lpc = pd.merge(alc_lpc, template, on=['location_id', 'year_id'], how='right')\n",
    "alc_lpc = pd.merge(alc_lpc, sub, on=['location_name', 'location_id', 'year_id'], how='left')\n",
    "\n",
    "alc_lpc = alc_lpc[['me_name', 'location_name', 'location_id', 'year_id']+draws+draws_old]\n",
    "alc_lpc['me_name'] = 'drugs_alcohol_lpc'\n",
    "alc_lpc.fillna(0, inplace=True)\n",
    "\n",
    "alc_lpc[draws] = alc_lpc[draws_old].values - alc_lpc[draws].values\n",
    "\n",
    "#Replace some countries with old estimates, due to tourism assumptions being violated\n",
    "check = alc_lpc[(alc_lpc[draws]<=0).any(axis=1)]\n",
    "countries = set(check['location_id'].values)\n",
    "for country in countries:\n",
    "    alc_lpc.loc[(alc_lpc['location_id'] == country), (draws)] = alc_lpc.loc[(alc_lpc['location_id'] == country), (draws_old)].values\n",
    "\n",
    "#44533 is the new China so drop location_id=6\n",
    "alc_lpc = alc_lpc[alc_lpc['location_id'] != 6]\n",
    "\n",
    "alc_lpc = alc_lpc[['me_name', 'location_name', 'location_id', 'year_id']+draws]\n",
    "alc_lpc.to_stata(r'J:\\WORK\\05_risk\\risks\\drugs_alcohol\\data\\exp\\stgpr\\alcohol_lpc.dta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
